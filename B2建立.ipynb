{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a9738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from time import time\n",
    "from GetPath import *\n",
    "import shutil\n",
    "spark = SparkSession.builder.appName('B2_Create').config('spark.dynamicAllocation.enabled','true').config('spark.debug.maxToStringFields', '100').config('spark.sql.execution.arrow.enable','true').config('spark.executor.memory','48g').config('spark.driver.memory', '48g').config('spark.core.connection.ack.wait.timeout','36000s').config('spark.executor.heartbeatInterval','36000s').config('spark.network.timeout', '50000s').config('spark.rpc.lookupTimeout', '5000s').config('spark.shuffle.io.connectionTimeout', '50000s').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80fd119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "input_path='D:/Output/chenbingying/B1/'\n",
    "output_path='D:/Output/chenbingying/B2/'\n",
    "A2_path='D:/Output/chenbingying/A2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84ae6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义判断是否广州的udf函数\n",
    "def Is_GuangZhou(x):\n",
    "    if(x is not None):\n",
    "        for i in ['广州','本市','越秀','海珠','荔湾','天河','白云','黄埔','南沙','番禺','花都','增城','从化','东山','芳村','萝岗']:\n",
    "            if i in x:\n",
    "                return '广州市'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3eaf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义区域字段udf\n",
    "def DISTRICT(x):\n",
    "    if(x is not None):\n",
    "        for i in [['芳村区','荔湾区'],['花县','花都区'],['东山区','越秀区'],['萝岗区','黄埔区']]:\n",
    "            x=x.replace(i[0],i[1])   \n",
    "        for i in ['荔湾','越秀','海珠','天河','白云','黄埔','番禺','花都','南沙','从化','增城']:\n",
    "            if i in x:\n",
    "                return i+'区'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0773ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义众数原则udf\n",
    "def return_mode(list1):\n",
    "    try:\n",
    "        if list1 is not None: \n",
    "            if len(list1)==1:\n",
    "                return list1[0]\n",
    "            b = {}\n",
    "            for i in list1:\n",
    "                if i in b.keys():\n",
    "                    b[i] += 1\n",
    "                else:\n",
    "                    b[i] = 1\n",
    "            c = sorted(b.items(), key=lambda x:x[1], reverse=True)\n",
    "            return c[0][0]\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08cd839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_B2(input_path,output_path,A2_path):\n",
    "    t_start=time()\n",
    "    filelist=GetPath(input_path,'tb_cis_op_visiting_record')\n",
    "    # 注册函数\n",
    "    spark.udf.register(\"Is_GuangZhou\",Is_GuangZhou,StringType())\n",
    "    spark.udf.register(\"DISTRICT\",DISTRICT,StringType())\n",
    "    spark.udf.register(\"return_mode\",return_mode,StringType()) \n",
    "    # A2表预处理\n",
    "    print('开始处理：A2表')\n",
    "    start=time()\n",
    "    spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(A2_path).createOrReplaceTempView(\"A2_Raw\")\n",
    "    # 取众数，确保患者信息表机构号加档案号唯一\n",
    "    spark.sql(\"\"\"\n",
    "    select MEDICAL_INSTITUTION_CODE\n",
    "    ,FILE_NUMBER \n",
    "    ,return_mode(collect_list(CARD_NUMBER)) as CARD_NUMBER\n",
    "    ,return_mode(collect_list(CARD_TYPE)) as CARD_TYPE\n",
    "    ,return_mode(collect_list(cast(BIRTH_DATE as string))) as BIRTH_DATE\n",
    "    ,return_mode(collect_list(GENDER)) as GENDER\n",
    "    ,return_mode(collect_list(CITY)) as CITY\n",
    "    ,return_mode(collect_list(DISTRICT)) as DISTRICT\n",
    "    from A2_Raw group by MEDICAL_INSTITUTION_CODE,FILE_NUMBER\n",
    "    \"\"\").createOrReplaceTempView(\"A2_result\")\n",
    "    # 取众数，确保患者信息表身份证号唯一\n",
    "    spark.sql(\"\"\"\n",
    "    select ID_NUMBER\n",
    "    ,return_mode(collect_list(CARD_NUMBER)) as CARD_NUMBER\n",
    "    ,return_mode(collect_list(CARD_TYPE)) as CARD_TYPE\n",
    "    ,return_mode(collect_list(cast(BIRTH_DATE as string))) as BIRTH_DATE\n",
    "    ,return_mode(collect_list(GENDER)) as GENDER\n",
    "    ,return_mode(collect_list(CITY)) as CITY\n",
    "    ,return_mode(collect_list(DISTRICT)) as DISTRICT\n",
    "    from A2_Raw group by ID_NUMBER\n",
    "    \"\"\").createOrReplaceTempView(\"A2_result_sfz\")\n",
    "    print('A2表预处理完毕\\n')\n",
    "    print('开始处理：B2表')\n",
    "    # B2表匹配\n",
    "    for file in filelist:\n",
    "        start=time()\n",
    "        print('开始处理',file.split('/')[-1].split('.csv')[0])\n",
    "        spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(file).createOrReplaceTempView(\"B2_Raw\")\n",
    "        print('读入数据耗时：%.2f秒'%(time()-start))\n",
    "        # B2表年龄、性别、是否广州字段补全，创建临时表B2_1\n",
    "        start=time()\n",
    "########################################################################        \n",
    "        spark.sql(\"\"\"\n",
    "        select\n",
    "        MEDICAL_INSTITUTION_CODE\n",
    "        ,FILE_NUMBER\n",
    "        ,ID_NUMBER\n",
    "        ,CARD_NUMBER\n",
    "        ,CARD_TYPE\n",
    "        ,AGE\n",
    "        ,GENDER \n",
    "        ,BIRTH_DATE\n",
    "        ,case when VISITING_DATE is null then DATE_OF_DIAGNOSIS else VISITING_DATE end as VISITING_DATE\n",
    "        ,DISEASE_DIAGNOSIS_CODE\n",
    "        ,VISITING_TYPE_CODE\n",
    "        ,VISITING_TYPE_NAME\n",
    "        ,ICD_CODE\n",
    "        ,AREA\n",
    "        ,id_birth_date\n",
    "        ,id_gender\n",
    "        from B2_Raw\n",
    "        \"\"\").createOrReplaceTempView(\"B2_0\")\n",
    "    \n",
    "        spark.sql(\"\"\"\n",
    "        select \n",
    "        MEDICAL_INSTITUTION_CODE\n",
    "        ,FILE_NUMBER\n",
    "        ,ID_NUMBER\n",
    "        ,CARD_NUMBER\n",
    "        ,CARD_TYPE\n",
    "        ,case when id_birth_date is not null and VISITING_DATE is not null then year(VISITING_DATE)-year(id_birth_date) \n",
    "              when id_birth_date is null and AGE is not null then AGE\n",
    "              when AGE is null and id_birth_date is null and BIRTH_DATE is not null and VISITING_DATE is not null then year(VISITING_DATE)-year(BIRTH_DATE)\n",
    "             else AGE end as AGE\n",
    "        ,case when id_gender is null and GENDER is not null then GENDER else id_gender end as GENDER\n",
    "        ,VISITING_DATE\n",
    "        ,DISEASE_DIAGNOSIS_CODE\n",
    "        ,VISITING_TYPE_CODE\n",
    "        ,VISITING_TYPE_NAME\n",
    "        ,ICD_CODE\n",
    "        ,Is_GuangZhou(AREA) as CITY\n",
    "        ,DISTRICT(AREA) as DISTRICT \n",
    "        from B2_0\n",
    "        \"\"\").createOrReplaceTempView(\"B2_1\")\n",
    "#########################################################################        \n",
    "#         spark.sql(\"\"\"\n",
    "#         select MEDICAL_INSTITUTION_CODE\n",
    "#         ,FILE_NUMBER\n",
    "#         ,ID_NUMBER\n",
    "#         ,CARD_NUMBER\n",
    "#         ,CARD_TYPE\n",
    "#        ,case when id_birth_date is not null and VISITING_DATE is not null then year(VISITING_DATE)-year(id_birth_date) \n",
    "#               when id_birth_date is null and AGE is not null then AGE\n",
    "#               when AGE is null and id_birth_date is null and BIRTH_DATE is not null and VISITING_DATE is not null then year(VISITING_DATE)-year(BIRTH_DATE)\n",
    "#              else AGE end as AGE\n",
    "#         ,case when id_gender is null and GENDER is not null then GENDER else id_gender end as GENDER\n",
    "#         ,case when VISITING_DATE is null then DATE_OF_DIAGNOSIS else VISITING_DATE end as VISITING_DATE\n",
    "#         ,DISEASE_DIAGNOSIS_CODE\n",
    "#         ,VISITING_TYPE_CODE\n",
    "#         ,VISITING_TYPE_NAME\n",
    "#         ,ICD_CODE\n",
    "#         ,Is_GuangZhou(AREA) as CITY\n",
    "#         ,DISTRICT(AREA) as DISTRICT \n",
    "#         from B2_Raw\n",
    "#         \"\"\").createOrReplaceTempView(\"B2_1\")\n",
    "        # B2表和A2表根据档案号和机构号匹配，创建临时表 match_01 ,flag字段为1表示匹配到，为0表示未匹配\n",
    "        spark.sql(\"\"\"\n",
    "        select b.MEDICAL_INSTITUTION_CODE\n",
    "        ,b.FILE_NUMBER\n",
    "        ,b.ID_NUMBER\n",
    "        ,b.CARD_NUMBER\n",
    "        ,b.CARD_TYPE\n",
    "        ,case when b.AGE is null and a.BIRTH_DATE is not null and b.VISITING_DATE is not null then year(b.VISITING_DATE)-year(a.BIRTH_DATE) else b.AGE end as AGE\n",
    "        ,case when b.GENDER is null then a.GENDER else b.GENDER end as GENDER\n",
    "        ,b.VISITING_DATE\n",
    "        ,b.DISEASE_DIAGNOSIS_CODE\n",
    "        ,b.VISITING_TYPE_CODE\n",
    "        ,b.VISITING_TYPE_NAME \n",
    "        ,b.ICD_CODE\n",
    "        ,case when b.CITY is not null then b.CITY else a.CITY end as CITY\n",
    "        ,case when b.DISTRICT is not null then b.DISTRICT else a.DISTRICT end as DISTRICT\n",
    "        ,case when a.MEDICAL_INSTITUTION_CODE is not null and a.FILE_NUMBER is not null then 1 else 0 end as flag\n",
    "        from B2_1 b left join A2_result a \n",
    "        on b.MEDICAL_INSTITUTION_CODE=a.MEDICAL_INSTITUTION_CODE and b.FILE_NUMBER=a.FILE_NUMBER\n",
    "        \"\"\").createOrReplaceTempView(\"match_01\")\n",
    "        # 使用身份证匹配\n",
    "        spark.sql(\"\"\"\n",
    "        select \n",
    "        b.MEDICAL_INSTITUTION_CODE\n",
    "        ,b.FILE_NUMBER\n",
    "        ,b.ID_NUMBER\n",
    "        ,b.CARD_NUMBER\n",
    "        ,b.CARD_TYPE\n",
    "        ,case when b.AGE is null and a.BIRTH_DATE is not null and b.VISITING_DATE is not null then year(b.VISITING_DATE)-year(a.BIRTH_DATE) else b.AGE end as AGE\n",
    "        ,case when b.GENDER is null then a.GENDER else b.GENDER end as GENDER \n",
    "        ,b.VISITING_DATE\n",
    "        ,b.DISEASE_DIAGNOSIS_CODE\n",
    "        ,b.VISITING_TYPE_CODE\n",
    "        ,b.VISITING_TYPE_NAME \n",
    "        ,b.ICD_CODE\n",
    "        ,case when b.CITY is not null then b.CITY else a.CITY end as CITY\n",
    "        ,case when b.DISTRICT is not null then b.DISTRICT else a.DISTRICT end as DISTRICT\n",
    "        from (select * from match_01 where flag=0) b left join A2_result_sfz a on \n",
    "        b.ID_NUMBER=a.ID_NUMBER\n",
    "        \"\"\").createOrReplaceTempView(\"match_02\")\n",
    "        # 合并结果表\n",
    "        spark.sql(\"\"\"\n",
    "        select \n",
    "        MEDICAL_INSTITUTION_CODE\n",
    "        ,FILE_NUMBER\n",
    "        ,ID_NUMBER\n",
    "        ,CARD_NUMBER\n",
    "        ,CARD_TYPE\n",
    "        ,AGE\n",
    "        ,GENDER\n",
    "        ,VISITING_DATE\n",
    "        ,DISEASE_DIAGNOSIS_CODE\n",
    "        ,VISITING_TYPE_CODE\n",
    "        ,VISITING_TYPE_NAME\n",
    "        ,ICD_CODE\n",
    "        ,CITY\n",
    "        ,DISTRICT\n",
    "        from match_01 where flag=1\n",
    "        union all\n",
    "        select * from match_02\n",
    "        \"\"\").coalesce(1).write.csv(output_path+file.split('/')[-1].split('.csv')[0],mode='overwrite',header=True)\n",
    "        shutil.move(GetPath(output_path+file.split('/')[-1].split('.csv')[0]+'/','csv'),output_path+file.split('/')[-1].split('.csv')[0]+'.csv')\n",
    "        shutil.rmtree(output_path+file.split('/')[-1].split('.csv')[0])\n",
    "        print('处理数据耗时：%.2f秒'%(time()-start))\n",
    "        print('已输出至：'+output_path+file.split('/')[-1].split('.csv')[0]+'.csv')\n",
    "        # 释放资源\n",
    "        spark.catalog.dropTempView(\"visiting_B2\")\n",
    "        spark.catalog.dropTempView(\"visiting_B2_1\")\n",
    "        spark.catalog.dropTempView(\"match_01\")\n",
    "        spark.catalog.dropTempView(\"match_02\")\n",
    "    spark.catalog.dropTempView(\"A2_Raw\")\n",
    "    spark.catalog.dropTempView(\"A2_result\")\n",
    "    spark.catalog.dropTempView(\"A2_result_sfz\")\n",
    "    print('总耗时：%.2f秒'%(time()-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd4d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理：A2表\n",
      "A2表预处理完毕\n",
      "\n",
      "开始处理：B2表\n",
      "开始处理 tb_cis_op_visiting_record_201401_201806\n",
      "读入数据耗时：94.11秒\n",
      "处理数据耗时：1824.66秒\n",
      "已输出至：D:/Output/zhuhao/B2/tb_cis_op_visiting_record_201401_201806.csv\n",
      "开始处理 tb_cis_op_visiting_record_201807_201912\n",
      "读入数据耗时：66.66秒\n",
      "处理数据耗时：1490.82秒\n",
      "已输出至：D:/Output/zhuhao/B2/tb_cis_op_visiting_record_201807_201912.csv\n",
      "开始处理 tb_cis_op_visiting_record_202001_202012\n",
      "读入数据耗时：88.35秒\n",
      "处理数据耗时：1702.06秒\n",
      "已输出至：D:/Output/zhuhao/B2/tb_cis_op_visiting_record_202001_202012.csv\n",
      "开始处理 tb_cis_op_visiting_record_202101_202112\n",
      "读入数据耗时：82.84秒\n",
      "处理数据耗时：1590.23秒\n",
      "已输出至：D:/Output/zhuhao/B2/tb_cis_op_visiting_record_202101_202112.csv\n",
      "开始处理 tb_cis_op_visiting_record_202201_202205\n",
      "读入数据耗时：44.64秒\n",
      "处理数据耗时：1113.94秒\n",
      "已输出至：D:/Output/zhuhao/B2/tb_cis_op_visiting_record_202201_202205.csv\n",
      "总耗时：8251.29秒\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Create_B2(input_path,output_path,A2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc407d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
